{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Regression\n",
        "##\n",
        "### Question 1: What is Simple Linear Regression?\n",
        "\n",
        "**Simple Linear Regression** is a statistical technique used to model the relationship between two continuous variables — one **independent variable (X)** and one **dependent variable (Y)**. It assumes a linear relationship of the form:<br>\n",
        "[\n",
        "Y = β₀ + β₁X + ε\n",
        "]\n",
        "where:\n",
        "\n",
        "* ( β₀ ) = intercept,\n",
        "* ( β₁ ) = slope,\n",
        "* ( ε ) = error term.\n",
        "  It helps predict the value of Y based on a given value of X.\n",
        "\n",
        "##\n",
        "\n",
        "### Question 2: What are the key assumptions of Simple Linear Regression?\n",
        "1. **Linearity** – The relationship between X and Y is linear.\n",
        "2. **Independence** – Observations are independent of each other.\n",
        "3. **Homoscedasticity** – Constant variance of residuals across all levels of X.\n",
        "4. **Normality** – Residuals are normally distributed.\n",
        "5. **No multicollinearity** – (only relevant when multiple predictors are used).\n",
        "\n",
        "##\n",
        "\n",
        "### Question 3: What is heteroscedasticity, and why is it important to address in regression models?\n",
        "\n",
        "**Heteroscedasticity** occurs when the **variance of residuals is not constant** across all levels of the independent variable(s).\n",
        "It violates the homoscedasticity assumption of regression and leads to:\n",
        "\n",
        "* **Inefficient estimates** of coefficients.\n",
        "* **Inaccurate standard errors**, making hypothesis tests unreliable.\n",
        "  Addressing it ensures valid statistical inference and model reliability.\n",
        "\n",
        "##\n",
        "\n",
        "### Question 4: What is Multiple Linear Regression?\n",
        "**Multiple Linear Regression (MLR)** models the relationship between one dependent variable (Y) and **two or more independent variables** (X₁, X₂, …, Xₙ).\n",
        "The general form is:<br><br>\n",
        "[\n",
        "Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ + ε\n",
        "]<br><br>\n",
        "It helps assess how multiple factors collectively influence a single outcome.\n",
        "\n",
        "##\n",
        "\n",
        "### Question 5: What is polynomial regression, and how does it differ from linear regression?\n",
        "\n",
        "**Polynomial Regression** models the relationship between X and Y using **higher-degree terms of X** (e.g., X², X³) to capture curvature.\n",
        "It differs from simple linear regression, which assumes a **straight-line relationship**.\n",
        "Example:<br><br>\n",
        "[\n",
        "Y = β₀ + β₁X + β₂X² + ε\n",
        "]<br><br>\n",
        "Polynomial regression can model **non-linear relationships** between variables.\n",
        "\n",
        "##\n",
        "\n",
        "### Question 6: Implement a Python program to fit a Simple Linear Regression model to the following sample data:\n",
        "* X = [1, 2, 3, 4, 5]\n",
        "* Y = [2.1, 4.3, 6.1, 7.9, 10.2]\n",
        "#### Plot the regression line over the data points.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2.1, 4.3, 6.1, 7.9, 10.2])\n",
        "\n",
        "# Model fitting\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Predictions\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, Y, color='blue', label='Data Points')\n",
        "plt.plot(X, Y_pred, color='red', label='Regression Line')\n",
        "plt.title(\"Simple Linear Regression\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Coefficient:\", model.coef_[0])\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "    Output: Coefficient: 1.9800000000000004\n",
        "            Intercept: 0.17999999999999794\n",
        "##\n",
        "\n",
        "### Question 7: Fit a Multiple Linear Regression model on this sample data:\n",
        "* Area = [1200, 1500, 1800, 2000]\n",
        "* Rooms = [2, 3, 3, 4]\n",
        "* Price = [250000, 300000, 320000, 370000]\n",
        "#### Check for multicollinearity using VIF and report the results.\n",
        "\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Data\n",
        "data = pd.DataFrame({\n",
        "    'Area': [1200, 1500, 1800, 2000],\n",
        "    'Rooms': [2, 3, 3, 4],\n",
        "    'Price': [250000, 300000, 320000, 370000]\n",
        "})\n",
        "\n",
        "X = data[['Area', 'Rooms']]\n",
        "y = data['Price']\n",
        "\n",
        "# Add constant for intercept\n",
        "X_const = sm.add_constant(X)\n",
        "model = sm.OLS(y, X_const).fit()\n",
        "\n",
        "# Calculate VIF\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Feature'] = X_const.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X_const.values, i)\n",
        "                   for i in range(X_const.shape[1])]\n",
        "\n",
        "print(model.summary())\n",
        "print(\"\\nVariance Inflation Factor:\\n\", vif_data)\n",
        "\n",
        "✅ **Interpretation:**\n",
        "\n",
        "* **High VIF (>5)** indicates **multicollinearity** between predictors.\n",
        "* If VIF values are low, the model is stable.\n",
        "\n",
        "    Output:                   OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                  Price   R-squared:                       0.999\n",
        "Model:                            OLS   Adj. R-squared:                  0.996\n",
        "Method:                 Least Squares   F-statistic:                     351.0\n",
        "Date:                Fri, 17 Oct 2025   Prob (F-statistic):             0.0377\n",
        "Time:                        16:36:30   Log-Likelihood:                -35.242\n",
        "No. Observations:                   4   AIC:                             76.48\n",
        "Df Residuals:                       1   BIC:                             74.64\n",
        "Df Model:                           2                                         \n",
        "Covariance Type:            nonrobust                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
        "------------------------------------------------------------------------------\n",
        "const       1.032e+05   9488.293     10.872      0.058   -1.74e+04    2.24e+05\n",
        "Area          63.1579     14.886      4.243      0.147    -125.992     252.308\n",
        "Rooms       3.474e+04   6381.240      5.444      0.116   -4.63e+04    1.16e+05\n",
        "==============================================================================\n",
        "Omnibus:                          nan   Durbin-Watson:                   2.053\n",
        "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.554\n",
        "Skew:                          -0.154   Prob(JB):                        0.758\n",
        "Kurtosis:                       1.202   Cond. No.                     1.01e+04\n",
        "==============================================================================\n",
        "\n",
        "Notes:\n",
        "...\n",
        "   Feature        VIF\n",
        "0   const  34.210526\n",
        "1    Area   7.736842\n",
        "2   Rooms   7.736842\n",
        "##\n",
        "\n",
        "### Question 8: Implement polynomial regression on the following data:\n",
        "* X = [1, 2, 3, 4, 5]\n",
        "* Y = [2.2, 4.8, 7.5, 11.2, 14.7]\n",
        "#### Fit a 2nd-degree polynomial and plot the resulting curve\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])\n",
        "\n",
        "# Transform to polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "Y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, Y, color='blue')\n",
        "plt.plot(X, Y_pred, color='red')\n",
        "plt.title(\"Polynomial Regression (Degree 2)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.show()\n",
        "##\n",
        "\n",
        "### Question 9: Create a residuals plot for a regression model trained on this data:\n",
        "* X = [10, 20, 30, 40, 50]\n",
        "* Y = [15, 35, 40, 50, 65]\n",
        "#### Assess heteroscedasticity by examining the spread of residuals.\n",
        "\n",
        "# Data\n",
        "X = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)\n",
        "Y = np.array([15, 35, 40, 50, 65])\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "Y_pred = model.predict(X)\n",
        "residuals = Y - Y_pred\n",
        "\n",
        "# Residual plot\n",
        "plt.scatter(X, residuals, color='purple')\n",
        "plt.axhline(y=0, color='black', linestyle='--')\n",
        "plt.title(\"Residual Plot\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "✅ **Interpretation:**\n",
        "\n",
        "* If residuals are **randomly scattered**, variance is constant (no heteroscedasticity).\n",
        "* If residuals **spread out**, heteroscedasticity exists.\n",
        "##\n",
        "\n",
        "### Question 10: Imagine you are a data scientist working for a real estate company. You need to predict house prices using features like area, number of rooms, and location. However, you detect heteroscedasticity and multicollinearity in your regression model. Explain the steps you would take to address these issues and ensure a robust model.\n",
        "To ensure a robust regression model:\n",
        "\n",
        "**For Heteroscedasticity:**\n",
        "\n",
        "* Use **log or Box-Cox transformations** on the dependent variable.\n",
        "* Apply **Weighted Least Squares (WLS)** to handle unequal variance.\n",
        "* Use **robust standard errors** to adjust variance estimates.\n",
        "\n",
        "**For Multicollinearity:**\n",
        "\n",
        "* Examine **VIF values** and remove or combine highly correlated predictors.\n",
        "* Use **Principal Component Analysis (PCA)** or **Regularization methods** like **Ridge or Lasso Regression**.\n",
        "* Collect more data or transform variables if feasible."
      ],
      "metadata": {
        "id": "1V32yuzVvWzo"
      }
    }
  ]
}